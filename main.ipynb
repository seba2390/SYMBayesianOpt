{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-10T02:08:08.538797Z",
     "start_time": "2023-11-10T02:08:08.104838Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "from numpy.random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    def __init__(self, \n",
    "                 num_params:int, \n",
    "                 limits: list[tuple[float,float]]=None, \n",
    "                 parameters: np.ndarray[float]=None):\n",
    "        if not isinstance(num_params, int):\n",
    "            raise TypeError('num_params must be an integer.')\n",
    "        if num_params <= 0:\n",
    "            raise ValueError('num_params must be greater than 0.')\n",
    "        if limits is not None:\n",
    "            if len(limits) != num_params:\n",
    "                raise ValueError('list of limits should be of length \"num_params\". ')\n",
    "        if parameters is not None:\n",
    "            if len(parameters) != num_params:\n",
    "                raise ValueError('When explicitly provided, the number of parameters should be equal to \"num_params\". ')\n",
    "        self.num_params = num_params\n",
    "        self.limits = limits\n",
    "        self.parameters = parameters\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        if self.parameters is None:\n",
    "            raise RuntimeError(f' \"parameters\" attribute has not been set. ')\n",
    "        return self.parameters\n",
    "    \n",
    "    def set_parameters(self, parameters: np.ndarray[float]):\n",
    "        if len(parameters) != self.num_params:\n",
    "                raise ValueError('When explicitly provided, the number of parameters should be equal to \"num_params\". ')\n",
    "        self.parameters = parameters\n",
    "        \n",
    "    def set_random_parameters(self):\n",
    "        self.parameters = self.get_random_parameters()\n",
    "    \n",
    "    def get_random_parameters(self, distribution: str = 'uniform'):\n",
    "        if distribution == 'uniform':\n",
    "            if self.limits is not None:\n",
    "                self.parameters = np.array([uniform(low=limit[0], high=limit[1]) for limit in self.limits])\n",
    "            else:\n",
    "                self.parameters = np.array([uniform(low=-np.inf, high=np.inf) for _ in range(self.num_params)])\n",
    "            return self.get_parameters()\n",
    "        \n",
    "\n",
    "class CovarianceMatrix:\n",
    "    def __init__(self, kernel: Callable):\n",
    "        self.kernel = kernel\n",
    "        self.matrix = None\n",
    "        self.samples = []\n",
    "        \n",
    "    def update_matrix(self, new_samples: list[HyperParameters]):\n",
    "        # Updating samples\n",
    "        self.samples.extend(new_samples)\n",
    "        \n",
    "        # Filling matrix for first time\n",
    "        if self.matrix is None:\n",
    "            N = len(self.samples)\n",
    "            matrix = np.empty((N, N), dtype=float)\n",
    "            for i in range(N):\n",
    "                for j in range(i,N):\n",
    "                    matrix[i,j] = self.kernel(self.samples[i],self.samples[j])\n",
    "                    if i != j:\n",
    "                        matrix[j, i] = matrix[i, j]\n",
    "            self.matrix = matrix\n",
    "            \n",
    "        # Updating existing matrix \n",
    "        else:\n",
    "            N_new, N_old = len(new_samples), self.matrix.shape[0]\n",
    "            N_total = N_new + N_old\n",
    "            total_matrix = np.empty((N_total, N_total), dtype=float)\n",
    "            \n",
    "            # Updating total matrix w. old covariances\n",
    "            total_matrix[:N_old,:N_old] = self.matrix\n",
    "            \n",
    "            # Calculating covariances between old and new samples\n",
    "            for i in range(N_old):\n",
    "                for j in range(N_new):\n",
    "                    total_matrix[i, N_old + j] = self.kernel(self.samples[i], new_samples[j])\n",
    "                    total_matrix[N_old + j, i] = total_matrix[i, N_old + j]\n",
    "            \n",
    "            # Calculating covariances between new samples\n",
    "            for i in range(N_new):\n",
    "                for j in range(i, N_new):\n",
    "                    total_matrix[N_old + i, N_old + j] = self.kernel(new_samples[i], new_samples[j])\n",
    "                    if i != j:\n",
    "                        total_matrix[N_old + j, N_old + i] = total_matrix[N_old + i, N_old + j]\n",
    "                       \n",
    "    def get_matrix(self):\n",
    "        if self.matrix is None:\n",
    "            raise RuntimeError(f' \"matrix\" attribute has not yet been set. Method: \".update_matrix()\" must be called first. ')\n",
    "        return self.matrix\n",
    "    \n",
    "\n",
    "class GaussianProcess:\n",
    "    def __init__(self, kernel: Callable, optimization_function: Callable):\n",
    "        \n",
    "        self.optimization_function = optimization_function\n",
    "        self.kernel = kernel\n",
    "        self.covariance_matrix = CovarianceMatrix(kernel=self.kernel)\n",
    "        \n",
    "        # Define sample = x, then y=f(x) is sample_value \n",
    "        self.sample_values = []\n",
    "    \n",
    "    def add_samples(self, samples: list[HyperParameters]) -> None:\n",
    "        self.covariance_matrix.update_matrix(new_samples=samples)\n",
    "        for sample in samples:\n",
    "            self.sample_values.append(self.optimization_function(sample))\n",
    "    \n",
    "    def get_mean_and_variance(self, sample: HyperParameters) -> tuple[float, float]:\n",
    "        if self.covariance_matrix.matrix is None:\n",
    "            raise RuntimeError(f'No samples have been given yet - expecting call to method \".add_samples()\" first. ')\n",
    "        kernel_vector = np.array([self.kernel(old_sample, sample) for old_sample in self.covariance_matrix.samples])\n",
    "        inv_covar = np.linalg.inv(self.covariance_matrix.get_matrix())\n",
    "        mean = np.dot(kernel_vector, np.dot(inv_covar, np.array(self.sample_values)))\n",
    "        variance = self.kernel(sample,sample) - np.dot(kernel_vector, np.dot(inv_covar, np.array(kernel_vector)))\n",
    "        return mean, variance\n",
    "\n",
    "\n",
    "class BayesianOptimization:\n",
    "    def __init__(self, \n",
    "                 kernel: Callable, \n",
    "                 optimization_function: Callable,\n",
    "                 limits: list[tuple[float,float]]):\n",
    "        \n",
    "        self.limits = limits\n",
    "        self.kernel = kernel\n",
    "        self.optimization_function = optimization_function\n",
    "        self.gaussian_process = GaussianProcess(kernel=self.kernel, optimization_function=self.optimization_function)\n",
    "        self.best_sample, self.best_sample_value = None, None\n",
    "    \n",
    "    def acquisition_function(self, sample: HyperParameters) -> float:\n",
    "        \n",
    "        def probability_of_improvement(best_sample_value: float, mean: float, variance: float) -> float:\n",
    "            return norm.cdf((best_sample_value-mean)/np.sqrt(variance))\n",
    "        \n",
    "        mean, variance = self.gaussian_process.get_mean_and_variance(sample=sample)\n",
    "        return -probability_of_improvement(best_sample_value=self.best_sample_value, mean=mean, variance=variance)\n",
    "    \n",
    "    def optimize(self, N_warmup: int, N_optimize: int):\n",
    "        # Warm-up\n",
    "        samples = []\n",
    "        for random_sample in range(N_warmup):\n",
    "            params = HyperParameters(num_params=len(self.limits),limits=self.limits)\n",
    "            params.set_random_parameters()\n",
    "            samples.append(params)     \n",
    "        self.gaussian_process.add_samples(samples=samples)\n",
    "        best_idx = np.argmin(self.gaussian_process.sample_values)\n",
    "        self.best_sample, self.best_sample_value = samples[best_idx], self.gaussian_process.sample_values[best_idx]\n",
    "        \n",
    "        # Optimization        \n",
    "        def wrapper(x: np.ndarray[float]) -> float:\n",
    "            return self.acquisition_function(sample=HyperParameters(num_params=len(self.limits),\n",
    "                                                                    limits=self.limits,\n",
    "                                                                     parameters=x))\n",
    "        for optimization_sample in range(N_optimize):\n",
    "        \n",
    "            # argmin_x \n",
    "            init = HyperParameters(num_params=len(self.limits),limits=self.limits)\n",
    "            init.set_random_parameters()\n",
    "            res = minimize(fun=wrapper, x0=init.get_parameters().flatten(), method='COBYLA', bounds=self.limits)\n",
    "            x_i = HyperParameters(num_params=len(self.limits), limits=self.limits, parameters=res.x)\n",
    "            \n",
    "            # Update samples, sample-values and covariance matrix.\n",
    "            self.gaussian_process.add_samples(samples=[x_i])\n",
    "            \n",
    "            # Update best values \n",
    "            best_idx = np.argmin(self.gaussian_process.sample_values)\n",
    "            self.best_sample, self.best_sample_value = samples[best_idx], self.gaussian_process.sample_values[best_idx]\n",
    "            \n",
    "\n",
    "    \n",
    "def rbf_kernel(x1: HyperParameters, x2: HyperParameters, length_scale=1.0, sigma_f=1.0):\n",
    "    \"\"\"\n",
    "    Compute the Radial Basis Function (RBF) kernel between two d-dimensional vectors.\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the squared Euclidean distance between the vectors\n",
    "    sqdist = np.sum((x1.get_parameters() - x2.get_parameters())**2)\n",
    "    \n",
    "    # Compute the RBF kernel\n",
    "    return sigma_f**2 * np.exp(-0.5 / length_scale**2 * sqdist)\n",
    "        \n",
    "\n",
    "# Treating variables as hyperparameters as example        \n",
    "def rosenbrock(x: HyperParameters) -> float:\n",
    "    x1, x2 = x.get_parameters(), x.get_parameters()[1]\n",
    "    print(x1, x2)\n",
    "    return ((1 - x1) ** 2 + 100 * (x2-x1 ** 2) ** 2)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T02:08:08.553768Z",
     "start_time": "2023-11-10T02:08:08.539301Z"
    }
   },
   "id": "f908f6ee9cf25edd"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "limits = [(-3,3), (-4,4)]\n",
    "opt = BayesianOptimization(kernel=rbf_kernel, \n",
    "                           optimization_function=rosenbrock,\n",
    "                           limits=limits)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T02:08:08.560773Z",
     "start_time": "2023-11-10T02:08:08.554927Z"
    }
   },
   "id": "bb8ba7865d6bfeef"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.12267438 1.84642068] 1.8464206822678566\n",
      "[-1.34524117  1.31534607] 1.3153460661952447\n",
      "[ 0.64942322 -2.99415882] -2.994158822566299\n",
      "[-1.01012901  2.57280993] 2.572809928149888\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (3,3) and (4,) not aligned: 3 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mopt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mN_warmup\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mN_optimize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 157\u001B[0m, in \u001B[0;36mBayesianOptimization.optimize\u001B[0;34m(self, N_warmup, N_optimize)\u001B[0m\n\u001B[1;32m    155\u001B[0m init \u001B[38;5;241m=\u001B[39m HyperParameters(num_params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlimits),limits\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlimits)\n\u001B[1;32m    156\u001B[0m init\u001B[38;5;241m.\u001B[39mset_random_parameters()\n\u001B[0;32m--> 157\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mminimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfun\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwrapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mCOBYLA\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlimits\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    158\u001B[0m x_i \u001B[38;5;241m=\u001B[39m HyperParameters(num_params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlimits), limits\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlimits, parameters\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mx)\n\u001B[1;32m    160\u001B[0m \u001B[38;5;66;03m# Update samples, sample-values and covariance matrix.\u001B[39;00m\n",
      "File \u001B[0;32m~/.python_venvs/SYMBayesionOpt/lib/python3.10/site-packages/scipy/optimize/_minimize.py:716\u001B[0m, in \u001B[0;36mminimize\u001B[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001B[0m\n\u001B[1;32m    713\u001B[0m     res \u001B[38;5;241m=\u001B[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001B[38;5;241m=\u001B[39mcallback,\n\u001B[1;32m    714\u001B[0m                         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m    715\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m meth \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcobyla\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 716\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43m_minimize_cobyla\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfun\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconstraints\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    717\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mbounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    718\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m meth \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mslsqp\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    719\u001B[0m     res \u001B[38;5;241m=\u001B[39m _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001B[1;32m    720\u001B[0m                           constraints, callback\u001B[38;5;241m=\u001B[39mcallback, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
      "File \u001B[0;32m~/.python_venvs/SYMBayesionOpt/lib/python3.10/site-packages/scipy/optimize/_cobyla_py.py:35\u001B[0m, in \u001B[0;36msynchronized.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _module_lock:\n\u001B[0;32m---> 35\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.python_venvs/SYMBayesionOpt/lib/python3.10/site-packages/scipy/optimize/_cobyla_py.py:278\u001B[0m, in \u001B[0;36m_minimize_cobyla\u001B[0;34m(fun, x0, args, constraints, rhobeg, tol, maxiter, disp, catol, callback, bounds, **unknown_options)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_jac\u001B[39m(x, \u001B[38;5;241m*\u001B[39margs):\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 278\u001B[0m sf \u001B[38;5;241m=\u001B[39m \u001B[43m_prepare_scalar_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfun\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_jac\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcalcfc\u001B[39m(x, con):\n\u001B[1;32m    281\u001B[0m     f \u001B[38;5;241m=\u001B[39m sf\u001B[38;5;241m.\u001B[39mfun(x)\n",
      "File \u001B[0;32m~/.python_venvs/SYMBayesionOpt/lib/python3.10/site-packages/scipy/optimize/_optimize.py:383\u001B[0m, in \u001B[0;36m_prepare_scalar_function\u001B[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001B[0m\n\u001B[1;32m    379\u001B[0m     bounds \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39minf, np\u001B[38;5;241m.\u001B[39minf)\n\u001B[1;32m    381\u001B[0m \u001B[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001B[39;00m\n\u001B[1;32m    382\u001B[0m \u001B[38;5;66;03m# calculation reduces overall function evaluations.\u001B[39;00m\n\u001B[0;32m--> 383\u001B[0m sf \u001B[38;5;241m=\u001B[39m \u001B[43mScalarFunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfun\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhess\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mfinite_diff_rel_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepsilon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sf\n",
      "File \u001B[0;32m~/.python_venvs/SYMBayesionOpt/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:158\u001B[0m, in \u001B[0;36mScalarFunction.__init__\u001B[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001B[0m\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;241m=\u001B[39m fun_wrapped(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx)\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_fun_impl \u001B[38;5;241m=\u001B[39m update_fun\n\u001B[0;32m--> 158\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_fun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;66;03m# Gradient evaluation\u001B[39;00m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(grad):\n",
      "File \u001B[0;32m~/.python_venvs/SYMBayesionOpt/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:251\u001B[0m, in \u001B[0;36mScalarFunction._update_fun\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_update_fun\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    250\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_updated:\n\u001B[0;32m--> 251\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_fun_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    252\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_updated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/.python_venvs/SYMBayesionOpt/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:155\u001B[0m, in \u001B[0;36mScalarFunction.__init__.<locals>.update_fun\u001B[0;34m()\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate_fun\u001B[39m():\n\u001B[0;32m--> 155\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;241m=\u001B[39m \u001B[43mfun_wrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.python_venvs/SYMBayesionOpt/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:137\u001B[0m, in \u001B[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnfev \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;66;03m# Send a copy because the user may overwrite it.\u001B[39;00m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;66;03m# Overwriting results in undefined behaviour because\u001B[39;00m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001B[39;00m\n\u001B[0;32m--> 137\u001B[0m fx \u001B[38;5;241m=\u001B[39m \u001B[43mfun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;66;03m# Make sure the function returns a true scalar\u001B[39;00m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39misscalar(fx):\n",
      "Cell \u001B[0;32mIn[2], line 149\u001B[0m, in \u001B[0;36mBayesianOptimization.optimize.<locals>.wrapper\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(x: np\u001B[38;5;241m.\u001B[39mndarray[\u001B[38;5;28mfloat\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[0;32m--> 149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquisition_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mHyperParameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlimits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[43m                                                            \u001B[49m\u001B[43mlimits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlimits\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[43m                                                             \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 133\u001B[0m, in \u001B[0;36mBayesianOptimization.acquisition_function\u001B[0;34m(self, sample)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprobability_of_improvement\u001B[39m(best_sample_value: \u001B[38;5;28mfloat\u001B[39m, mean: \u001B[38;5;28mfloat\u001B[39m, variance: \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m norm\u001B[38;5;241m.\u001B[39mcdf((best_sample_value\u001B[38;5;241m-\u001B[39mmean)\u001B[38;5;241m/\u001B[39mnp\u001B[38;5;241m.\u001B[39msqrt(variance))\n\u001B[0;32m--> 133\u001B[0m mean, variance \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgaussian_process\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_mean_and_variance\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39mprobability_of_improvement(best_sample_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_sample_value, mean\u001B[38;5;241m=\u001B[39mmean, variance\u001B[38;5;241m=\u001B[39mvariance)\n",
      "Cell \u001B[0;32mIn[2], line 111\u001B[0m, in \u001B[0;36mGaussianProcess.get_mean_and_variance\u001B[0;34m(self, sample)\u001B[0m\n\u001B[1;32m    109\u001B[0m kernel_vector \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel(old_sample, sample) \u001B[38;5;28;01mfor\u001B[39;00m old_sample \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcovariance_matrix\u001B[38;5;241m.\u001B[39msamples])\n\u001B[1;32m    110\u001B[0m inv_covar \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39minv(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcovariance_matrix\u001B[38;5;241m.\u001B[39mget_matrix())\n\u001B[0;32m--> 111\u001B[0m mean \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(kernel_vector, \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43minv_covar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample_values\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    112\u001B[0m variance \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel(sample,sample) \u001B[38;5;241m-\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(kernel_vector, np\u001B[38;5;241m.\u001B[39mdot(inv_covar, np\u001B[38;5;241m.\u001B[39marray(kernel_vector)))\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m mean, variance\n",
      "\u001B[0;31mValueError\u001B[0m: shapes (3,3) and (4,) not aligned: 3 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "opt.optimize(N_warmup=3,N_optimize=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T02:08:09.076810Z",
     "start_time": "2023-11-10T02:08:08.590951Z"
    }
   },
   "id": "52ac0b0096744fb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(opt.gaussian_process.covariance_matrix.get_matrix())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T02:08:09.080512Z",
     "start_time": "2023-11-10T02:08:09.078471Z"
    }
   },
   "id": "cf6d6bc4399fe988"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opt.gaussian_process.sample_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T02:08:09.080473Z"
    }
   },
   "id": "a5552536436fb37e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T02:08:09.081572Z"
    }
   },
   "id": "167fa2f05814b71e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
